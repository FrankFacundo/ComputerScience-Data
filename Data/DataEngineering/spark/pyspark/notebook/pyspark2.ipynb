{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/05 19:35:54 WARN Utils: Your hostname, frank-xps resolves to a loopback address: 127.0.1.1; using 192.168.1.63 instead (on interface wlp0s20f3)\n",
      "23/11/05 19:35:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/05 19:35:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Works when master and worker are launched.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"My Spark Application\") \\\n",
    "    .master(\"spark://frank-xps:7077\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "75000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download dataset from: https://drive.google.com/file/d/1kCXnIeoPT6p9kS_ANJ0mmpxlfDwK1yio/view\n",
    "path_dataset = '/media/frank/My Passport/datalab/datasets/big_dataset'\n",
    "products_table = spark.read.parquet(path_dataset + \"/products_parquet\")\n",
    "products_table.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7497262"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_table.sample(0.1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataframeReducer:\n",
    "    '''\n",
    "    Python2 complying\n",
    "    * This class will have a bad behavior if it is used in a concurrent way over the same table.\n",
    "    '''\n",
    "    REDUCED_SUFFIX = \"reduced\"\n",
    "    ORIGINAL_SUFFIX = \"original\"\n",
    "\n",
    "    def __init__(self, base_dir, format_file='orc', number_partition_output=1, hdfs=True):\n",
    "        self.base_dir = base_dir\n",
    "        self.format_file = format_file\n",
    "        self.hdfs = hdfs\n",
    "        self.number_partition_output = number_partition_output\n",
    "\n",
    "    def sample_with_replacement(self, table_name, percentage):\n",
    "        '''\n",
    "        Sample original table and put it into working path (without suffix)\n",
    "        '''\n",
    "        is_original_table_present = self.check_original_dataframe_exists(table_name)\n",
    "        if not is_original_table_present:\n",
    "            self.move_from_current_to_original(table_name)\n",
    "        print(\"***** Sampling... *****\")\n",
    "        self.sample(table_name, percentage)\n",
    "        print(\"***** Putting reduced table into working table... ***** \")\n",
    "        self.move_from_reduced_to_current(table_name)\n",
    "\n",
    "    def sample(self, table_name, percentage):\n",
    "        try:\n",
    "            if percentage <= 0 or percentage >=1:\n",
    "                raise AssertionError(\"Percentage must be a float between excluded 0 and 100.\")\n",
    "            input_path = os.path.join(self.base_dir, \"{}_{}\".format(table_name, DataframeReducer.ORIGINAL_SUFFIX))\n",
    "            output_path = os.path.join(self.base_dir, \"{}_{}\".format(table_name, DataframeReducer.REDUCED_SUFFIX))\n",
    "            df_input = spark.read.format(self.format_file).load(input_path)\n",
    "            print(\"Total number of rows: {}\".format(df_input.count()))\n",
    "            print(\"Dataframe is written at : {}\".format(output_path))\n",
    "            print(\"Dataframe output format is : {}\".format(self.format_file))\n",
    "            df_output = df_input.sample(fraction=percentage/100, withReplacement=False, seed=None)\n",
    "            print(\"Reduced table partitions: {}\".format(self.number_partition_output))\n",
    "            print(\"Reduced table number of rows: {}\".format(df_output.count()))\n",
    "            df_output.coalesce(self.number_partition_output).write.format(self.format_file).mode('overwrite').save(output_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    def reset(self, table_name):\n",
    "        is_original_table_present = self.check_original_dataframe_exists(table_name)\n",
    "        if is_original_table_present:\n",
    "            self.move_from_original_to_current(table_name)\n",
    "        self.remove_dataframe(\"{}_{}\".format(table_name, DataframeReducer.REDUCED_SUFFIX))\n",
    "\n",
    "    def move_from_original_to_current(self, table_name):\n",
    "        self.remove_dataframe(table_name)\n",
    "        self.move_dataframe(\"{}_{}\".format(table_name, DataframeReducer.ORIGINAL_SUFFIX), table_name)\n",
    "\n",
    "    def move_from_reduced_to_current(self, table_name):\n",
    "        self.remove_dataframe(table_name)\n",
    "        self.move_dataframe(\"{}_{}\".format(table_name, DataframeReducer.REDUCED_SUFFIX), table_name)\n",
    "\n",
    "    def move_from_current_to_original(self, table_name):\n",
    "        self.remove_dataframe(\"{}_{}\".format(table_name, DataframeReducer.ORIGINAL_SUFFIX))\n",
    "        self.move_dataframe(table_name, \"{}_{}\".format(table_name, DataframeReducer.ORIGINAL_SUFFIX))\n",
    "\n",
    "    def check_original_dataframe_exists(self, table_name):\n",
    "        original_dataframe_path = os.path.join(self.base_dir, \"{}_{}\".format(table_name, DataframeReducer.ORIGINAL_SUFFIX))\n",
    "        is_original_table_present = self.check_dataframe_exists(original_dataframe_path)\n",
    "        return is_original_table_present\n",
    "\n",
    "    def check_dataframe_exists(self, table_name):\n",
    "        if not self.hdfs:\n",
    "            table_path = os.path.join(self.base_dir, table_name)\n",
    "            is_table_present = os.path.isdir(table_path)\n",
    "            return is_table_present\n",
    "\n",
    "    def remove_dataframe(self, table_name):\n",
    "        if not self.hdfs:\n",
    "            is_table_present = self.check_dataframe_exists(table_name)\n",
    "            if is_table_present:\n",
    "                dataframe_path = os.path.join(self.base_dir, table_name)            \n",
    "                try:\n",
    "                    shutil.rmtree(dataframe_path)\n",
    "                except OSError as e:\n",
    "                    print(\"Error: %s : %s\" % (dataframe_path, e.strerror))\n",
    "    \n",
    "    def move_dataframe(self, table_name_src, table_name_dst):\n",
    "        source_path = os.path.join(self.base_dir, table_name_src)\n",
    "        destination_path = os.path.join(self.base_dir, table_name_dst)\n",
    "\n",
    "        if not self.hdfs:\n",
    "            shutil.move(source_path, destination_path)\n",
    "    \n",
    "    def copy_dataframe(self, table_name_src, table_name_dst):\n",
    "        source_path = os.path.join(self.base_dir, table_name_src)\n",
    "        destination_path = os.path.join(self.base_dir, table_name_dst)\n",
    "        if not self.hdfs:\n",
    "            shutil.copy(source_path, destination_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = \"./data\"\n",
    "format_file = \"parquet\"\n",
    "redu = DataframeReducer(output, format_file=format_file, hdfs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Sampling... *****\n",
      "Total number of rows: 75000000\n",
      "Dataframe is written at : /home/frank/Code/ComputerScience-Data/Data/DataEngineering/spark/pyspark/data/products_parquet_reduced\n",
      "Dataframe output format is : parquet\n",
      "Reduced table partitions: 1\n",
      "Reduced table number of rows: 75214\n",
      "***** Putting reduced table into working table... ***** \n"
     ]
    }
   ],
   "source": [
    "redu.sample_with_replacement(\"products_parquet\", 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "redu.reset(\"products_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75000000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_table = spark.read.parquet(\"./data/products_parquet\")\n",
    "products_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path does not exist: file:/home/frank/Code/ComputerScience-Data/Data/DataEngineering/spark/pyspark/data/products_parquet_original;\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    products_table_original = spark.read.parquet(\"./data/products_parquet_original\")\n",
    "    print(products_table_original.count())\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path does not exist: file:/home/frank/Code/ComputerScience-Data/Data/DataEngineering/spark/pyspark/data/products_parquet_reduced;\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    products_table_reduced = spark.read.parquet(\"./data/products_parquet_reduced\")\n",
    "    print(products_table_reduced.count())\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7.18 ('py2spark2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ecf21138c9b585ba5b7ba5fba9723a8e70d7fab353f7b8f6845281d73ae6768"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
