{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# One-to-many - Learning to generate text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda activate tf_p39\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "CHECKPOINT_DIR = os.path.join(DATA_DIR, \"checkpoints\")\n",
    "LOG_DIR = os.path.join(DATA_DIR, \"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_logs():\n",
    "    shutil.rmtree(CHECKPOINT_DIR, ignore_errors=True)\n",
    "    shutil.rmtree(LOG_DIR, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_read(urls):\n",
    "    texts : List[str] = []\n",
    "    for i, url in enumerate(urls):\n",
    "        p = tf.keras.utils.get_file(\"ex1-{:d}.txt\".format(i), url,\n",
    "                                    cache_dir=\".\")\n",
    "        text = open(p, mode=\"r\", encoding=\"utf-8\").read()\n",
    "        # remove byte order mark\n",
    "        text = text.replace(\"\\ufeff\", \"\")\n",
    "        # remove newlines\n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = re.sub(r'\\s+', \" \", text)\n",
    "        # add it to the list\n",
    "        texts.extend(text)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_labels(sequence):\n",
    "    input_seq = sequence[0:-1]\n",
    "    output_seq = sequence[1:]\n",
    "    return input_seq, output_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CharGenModel(vocab_size=92, num_timesteps=100, embedding_dim=256)\n",
    "class CharGenModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, num_timesteps,\n",
    "                 embedding_dim, **kwargs):\n",
    "        super(CharGenModel, self).__init__(**kwargs)\n",
    "        \"\"\"\n",
    "        input_dim: Integer. Size of the vocabulary, i.e. maximum integer index + 1.\n",
    "        output_dim: Integer. Dimension of the dense embedding.\n",
    "        \"\"\"\n",
    "        self.embedding_layer = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim\n",
    "        )\n",
    "        \"\"\"\n",
    "        stateful=True: The hidden state output from the previous \n",
    "            training epoch will be used as input to the current epoch.\n",
    "        return_sequences=True: The RNN will output at each of the \n",
    "            time steps rather than an aggregate output at the last time steps.\n",
    "        To get more details about GRU (Gated recurrent unit) check book Deep Learning with TensorFlow pg.146\n",
    "        - units: Positive integer, dimensionality of the output space.\n",
    "        - recurrent_initializer: Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state. Default: orthogonal.\n",
    "        - recurrent_activation: Activation function to use for the recurrent step. Default: sigmoid (sigmoid). If you pass None, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
    "        Doc: https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU\n",
    "        \"\"\"\n",
    "\n",
    "        self.rnn_layer = tf.keras.layers.GRU(\n",
    "            units=num_timesteps,\n",
    "            recurrent_initializer=\"glorot_uniform\",\n",
    "            recurrent_activation=\"sigmoid\",\n",
    "            stateful=True,\n",
    "            return_sequences=True\n",
    "        )\n",
    "        \"\"\"\n",
    "        units: Positive integer, dimensionality of the output space.\n",
    "        \"\"\"\n",
    "        self.dense_layer = tf.keras.layers.Dense(units=vocab_size)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding_layer(x)\n",
    "        x = self.rnn_layer(x)\n",
    "        x = self.dense_layer(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, predictions):\n",
    "    return tf.losses.sparse_categorical_crossentropy(\n",
    "        labels,\n",
    "        predictions,\n",
    "        from_logits=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, prefix_string: str, char2idx: Dict[str, int], idx2char: Dict[int, str],\n",
    "                  num_chars_to_generate=1000, temperature=1.0):\n",
    "    input = [char2idx[s] for s in prefix_string]\n",
    "    input = tf.expand_dims(input, 0)\n",
    "    text_generated = []\n",
    "    model.reset_states()\n",
    "    for i in range(num_chars_to_generate):\n",
    "        preds = model(input)\n",
    "        preds = tf.squeeze(preds, 0) / temperature\n",
    "        # predict char returned by model\n",
    "        # logits: 2-D Tensor with shape [batch_size, num_classes]. Each slice [i, :] represents the unnormalized log-probabilities for all classes.\n",
    "        pred_id = tf.random.categorical(logits=preds, num_samples=1)[-1, 0].numpy()\n",
    "        text_generated.append(idx2char[pred_id])\n",
    "        # pass the prediction as the next input to the model\n",
    "        input = tf.expand_dims([pred_id], 0)\n",
    "\n",
    "    return prefix_string + \"\".join(text_generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# download and read into local data structure (list of chars)\n",
    "texts = download_and_read([\n",
    "    \"http://www.gutenberg.org/cache/epub/28885/pg28885.txt\",\n",
    "    \"https://www.gutenberg.org/files/12/12-0.txt\"\n",
    "])\n",
    "clean_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: [' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '·', 'Æ', 'ù', '—', '‘', '’', '“', '”']\n",
      "vocab size: 92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create the vocabulary\n",
    "vocab = sorted(set(texts))\n",
    "print(\"vocab: {}\".format(vocab))\n",
    "print(\"vocab size: {:d}\\n\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping from vocab chars to ints\n",
    "char2idx: Dict[str, int] = {c: i for i, c in enumerate(vocab)}\n",
    "idx2char: Dict[int, str] = {i: c for c, i in char2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numericize the texts\n",
    "texts_as_ints: np.ndarray = np.array([char2idx[c] for c in texts])\n",
    "data = tf.data.Dataset.from_tensor_slices(texts_as_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset : <BatchDataset shapes: (101,), types: tf.int64>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of characters to show before asking for prediction\n",
    "# sequences: [None, 100]\n",
    "seq_length = 100\n",
    "sequences = data.batch(seq_length + 1, drop_remainder=True)\n",
    "print(\"dataset : {}\\n\".format(sequences))\n",
    "sequences = sequences.map(split_train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:[Project Gutenberg's Alice's Adventures in Wonderland, by Lewis Carroll This eBook is for the use of ]\n",
      "output:[roject Gutenberg's Alice's Adventures in Wonderland, by Lewis Carroll This eBook is for the use of a]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print out input and output to see what they look like\n",
    "for input_seq, output_seq in sequences.take(1):\n",
    "    print(\"input:[{:s}]\".format(\n",
    "        \"\".join([idx2char[i] for i in input_seq.numpy()])))\n",
    "    print(\"output:[{:s}]\\n\".format(\n",
    "        \"\".join([idx2char[i] for i in output_seq.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset : <BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set up for training\n",
    "# batches: [None, 64, 100]\n",
    "batch_size: int = 64\n",
    "steps_per_epoch: int = (len(texts) // seq_length) // batch_size\n",
    "dataset = sequences.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
    "print(\"dataset : {}\\n\".format(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"char_gen_model_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_26 (Embedding)    multiple                  23552     \n",
      "                                                                 \n",
      " gru_26 (GRU)                multiple                  107400    \n",
      "                                                                 \n",
      " dense_26 (Dense)            multiple                  9292      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 140,244\n",
      "Trainable params: 140,244\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define network\n",
    "vocab_size: int = len(vocab)\n",
    "embedding_dim: int = 256\n",
    "\n",
    "model = CharGenModel(vocab_size, seq_length, embedding_dim)\n",
    "model.build(input_shape=(batch_size, seq_length))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_batch.shape: (64, 100, 92)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# try running some data through the model to validate dimensions\n",
    "for input_batch, label_batch in dataset.take(1):\n",
    "    pred_batch = model(input_batch)\n",
    "\n",
    "print(\"pred_batch.shape: {}\\n\".format(pred_batch.shape))\n",
    "assert(pred_batch.shape[0] == batch_size)\n",
    "assert(pred_batch.shape[1] == seq_length)\n",
    "assert(pred_batch.shape[2] == vocab_size)\n",
    "\n",
    "model.compile(optimizer=tf.optimizers.Adam(), loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "54/54 [==============================] - 4s 65ms/step - loss: 1.8550\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 3s 62ms/step - loss: 1.8177\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 3s 62ms/step - loss: 1.7851\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 3s 63ms/step - loss: 1.7528\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 4s 67ms/step - loss: 1.7269\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 4s 73ms/step - loss: 1.7041\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 3s 60ms/step - loss: 1.6783\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 3s 60ms/step - loss: 1.6562\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 3s 60ms/step - loss: 1.6383\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 3s 60ms/step - loss: 1.6233\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).embedding_layer.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).embedding_layer.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "after epoch: 10\n",
      "Alice 5. Fibrived it make way a ring’r in a paying a of chorge, offout it in as with only, mus, I's are on replace—your heart heave it canched as to nuppting a projectle, hish a pillit should hem come on tillow. Ard This were shastsing!\" stiok! Nowaght alace of the hear that's RoRe Frork. What I’ve broon, here’s a by they wime unerousing and went on itsel sotion as cog, then esity thing, thing—dowly hange fouming of done would remingase sell shall said ither maiferoout) onling come the all rus. \"Nowing?” “It’s the emany. So paughtly asmeth you of I seems be think look to which The very the very she said. “But to be blectration _she mongurdered alse with its on a were the little and the right, and that oven to real arrwould precedenot on your mea’le ot sigh bereaticy quiting to aves for olaclable8, so law Alice chute.” And I were pletily chare, them, Wo don't by mreem, and by memost, full begins this alore canwitn eI tro pereme, and the ngon’ts herselooked veralking of the oveepare as put on \n",
      "---\n",
      "Epoch 1/10\n",
      "54/54 [==============================] - 4s 60ms/step - loss: 1.6079\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 3s 60ms/step - loss: 1.5895\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 3s 61ms/step - loss: 1.5774\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 3s 60ms/step - loss: 1.5634\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 3s 61ms/step - loss: 1.5539\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 3s 60ms/step - loss: 1.5441\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 3s 61ms/step - loss: 1.5276\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 3s 61ms/step - loss: 1.5231\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 3s 60ms/step - loss: 1.5115\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 3s 63ms/step - loss: 1.5062\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).embedding_layer.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).embedding_layer.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "after epoch: 20\n",
      "Alice stoo two Now.” “I won't her, I wondered in a nears, No cised to timmered it is began petches or hourth of the agne with the hopwore May to be birne--and alvound the ware beaince! Cat; \"if theg rupprees and fleas firth some nouse she Slat_ large cowasidy larg/gues,” He mean nerpilly, on it anything made a grine,” the Knig rooder: whe cares—and gradued going the know you middilys, and if it ever sone. \"Cown the took are you'--fried, thendity, and id tou’r of the Queen am make copen some beacenoth is pustan said), so quite re side of not works, and what rubbed leftiot. They long is the Angulation in a offilerary than entlen?” “It’s feet fidd. “I'm selousing ryplo!\" that_ while no called like,” they said tone. Inst I come by justly out his birive asoht.) Alice caind.\" \"You havenanis of the she mante as then So shantiftanie wnowhed he wordly to comined. And the questryingl: and headled. \"Then bike “Coldy inute down at the feey: sam, but one,” the King an’t pate beation _iberilar, I won the \n",
      "---\n",
      "Epoch 1/10\n",
      "54/54 [==============================] - 4s 61ms/step - loss: 1.4964\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 3s 62ms/step - loss: 1.4883\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 3s 61ms/step - loss: 1.4814\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 3s 62ms/step - loss: 1.4748\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 3s 61ms/step - loss: 1.4638\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 3s 63ms/step - loss: 1.4654\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 3s 61ms/step - loss: 1.4558\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 3s 60ms/step - loss: 1.4476\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 3s 61ms/step - loss: 1.4461\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 3s 64ms/step - loss: 1.4459\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).embedding_layer.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).embedding_layer.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "after epoch: 30\n",
      "Alice said? Frogrected away scrament. You'm which would surprise, Now,” the Lio_, I looking shaziss, who're which a wind fee'ver manning,\" said the Came_ keffed, bewlites?” “The Dormouse cast or undeed to growing,\" said Alice. \"I beginning alous thole, and be perton. \"Well, by then to yough!\" only down, for the bit of a rescoved to comse to be reave broop, buch sold striest date there went meneeply. Thatcemented: \"and slowing, beamed in here’ver and of it in the two began!\" Alice said, “it’s _quite means on upent e which his home putting evadions!” She managed to the queer of exmbuddezaming to timmer extyright am at its than it is chonithe was buring it: great me are if you: and it small. ” All went part I went on bean than felt proquestion Alice, all redment, what to this hands of times!\" \"You spilens, (ah renudderly ‘__then _it were in an! a dreadfularaty--and your next with hand Alice, as the dance cried to dink. How they shook, merulling ofly. \"Whronic it a Ensuch devent one heard. These\n",
      "---\n",
      "Epoch 1/10\n",
      "54/54 [==============================] - 4s 60ms/step - loss: 1.4362\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 3s 61ms/step - loss: 1.4307\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 3s 61ms/step - loss: 1.4257\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 3s 61ms/step - loss: 1.4202\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 3s 61ms/step - loss: 1.4188\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 3s 60ms/step - loss: 1.4154\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 3s 62ms/step - loss: 1.4091\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 3s 61ms/step - loss: 1.4044\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 3s 60ms/step - loss: 1.4018\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 3s 62ms/step - loss: 1.3969\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).embedding_layer.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).embedding_layer.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "after epoch: 40\n",
      "Alice to see these \"You're was ray-tever. “She’re! she saw they, _you_ heart going all as comes at the shrild your agree this ageeved back entibed and uponing do sight lime to trumber and eyes?” “I knew you runient and was are_ in up to every provide 4. Your kaid green.” “She's thinking that a trizels of your lefther! Welling formortuntamings spok the pegred, like the ‘a two Project Gutenberg-tm work and sheen the Dormanch down to this comp?\" thought ordoned. \"I don’t 4. $280080%0·1CHE11) 91w Noby _quios, that about of you?” said the Mader.\" \"They lew nother then the begain in ahought they to _thank yet this arm were like bround more fromment to see list of his face oldne eyes of all _I_, no ax-made of he did go of five too I must recars, that exaction, when _soment Gate. So she had both uncome none. I've that doesn’t us, three of a shouldn’t say in the thought there_?” thought Alice, \"you began beast! He thought it so she caustion of the going Dive Froghtorged with a little the work,” the G\n",
      "---\n",
      "Epoch 1/10\n",
      "54/54 [==============================] - 4s 62ms/step - loss: 1.3950\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 3s 61ms/step - loss: 1.3903\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 3s 60ms/step - loss: 1.3858\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 3s 61ms/step - loss: 1.3854\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 3s 62ms/step - loss: 1.3799\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 3s 61ms/step - loss: 1.3795\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 3s 63ms/step - loss: 1.3751\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 3s 61ms/step - loss: 1.3772\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 3s 61ms/step - loss: 1.3700\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 3s 61ms/step - loss: 1.3650\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).embedding_layer.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).embedding_layer.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "after epoch: 50\n",
      "Alice tell intleated to her have this armut,\" said Alice, but I _I_ wait minest the birdsed at last-of it course to do been provided we feet you made a movesly. “The look again, to tried was silent and sigh, but any oneremord. Down asoon on this wash!” “And nolk awcrens.” “On cried 1.F1.W. 1.C.H DIBY A Cappergar, which thought to her spoken, intain, of watch! ‘_CEntefslecs him, and.) “It fage and af nols enor golding turn of the 24120 ERS. All his tame frightened: and of any little moul at can, so the thing_] The Queen said, it’s a nearly incl there my happens well as he propive bit,\" said, “that this reas peried from the took “Mise; \"I must '?\" she solfistions the broken speak witell, into it, no was the wook by Just becaused bit: and her sixted as she go now.' ENow toon the nowerg she bestaining to don’t broke sting to begin as can't ter got to sithy.” “Till return that she hit at the tood I may my upall another none. \"Fraddlerroubsly: “am, other. “I’ll the suze, titn again a day. And the \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# we will train our model for 50 epochs, and after every 10 epochs\n",
    "# we want to see how well it will generate text\n",
    "num_epochs: int = 50\n",
    "for i in range(num_epochs // 10):\n",
    "    model.fit(\n",
    "        dataset.repeat(),\n",
    "        epochs=10,\n",
    "        steps_per_epoch=steps_per_epoch\n",
    "        # callbacks=[checkpoint_callback, tensorboard_callback]\n",
    "    )\n",
    "    checkpoint_file = os.path.join(\n",
    "        CHECKPOINT_DIR, \"model_epoch_{:d}\".format(i+1))\n",
    "    model.save_weights(checkpoint_file)\n",
    "\n",
    "    # create a generative model using the trained model so far\n",
    "    gen_model = CharGenModel(vocab_size, seq_length, embedding_dim)\n",
    "    gen_model.load_weights(checkpoint_file)\n",
    "    gen_model.build(input_shape=(1, seq_length))\n",
    "\n",
    "    print(\"after epoch: {:d}\".format((i+1)*10))\n",
    "    print(generate_text(gen_model, \"Alice \", char2idx, idx2char))\n",
    "    print(\"---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_p39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5927ddc4cb01a6ba059eb42a698e965db03a50bdd8d02bc22bb15431b0c99791"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
