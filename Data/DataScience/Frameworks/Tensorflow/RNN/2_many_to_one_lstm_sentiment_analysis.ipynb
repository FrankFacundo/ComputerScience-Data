{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda activate tf_p39\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_logs(data_dir):\n",
    "    logs_dir = os.path.join(data_dir, \"logs\")\n",
    "    shutil.rmtree(logs_dir, ignore_errors=True)\n",
    "    return logs_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_read(url):\n",
    "    local_file = url.split('/')[-1]\n",
    "    local_file = local_file.replace(\"%20\", \" \")\n",
    "    p = tf.keras.utils.get_file(local_file, url, \n",
    "        extract=True, cache_dir=\".\")\n",
    "    local_folder = os.path.join(\"datasets\", local_file.split('.')[0])\n",
    "    labeled_sentences = []\n",
    "    for labeled_filename in os.listdir(local_folder):\n",
    "        if labeled_filename.endswith(\"_labelled.txt\"):\n",
    "            with open(os.path.join(local_folder, labeled_filename), \"r\") as f:\n",
    "                for line in f:\n",
    "                    sentence, label = line.strip().split('\\t')\n",
    "                    labeled_sentences.append((sentence, label))\n",
    "    return labeled_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentimentAnalysisModel(vocab_size=5271, max_seqlen=64)\n",
    "class SentimentAnalysisModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, max_seqlen, **kwargs):\n",
    "        super(SentimentAnalysisModel, self).__init__(**kwargs)\n",
    "        \"\"\"\n",
    "        input_dim: Integer. Size of the vocabulary, i.e. maximum integer index + 1.\n",
    "        output_dim: Integer. Dimension of the dense embedding.\n",
    "        \"\"\"\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=max_seqlen)\n",
    "        \"\"\"\n",
    "        - units: Positive integer, dimensionality of the output space.\n",
    "        \"\"\"\n",
    "        self.bilstm = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(units=max_seqlen)\n",
    "        )\n",
    "        \"\"\"\n",
    "        - units: Positive integer, dimensionality of the output space.\n",
    "        \"\"\"\n",
    "        self.dense = tf.keras.layers.Dense(units=64, activation=\"relu\")\n",
    "        self.out = tf.keras.layers.Dense(units=1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.bilstm(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# clean up log area\n",
    "data_dir = \"./data\"\n",
    "logs_dir = clean_logs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and read data into data structures\n",
    "labeled_sentences = download_and_read(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\")\n",
    "sentences = [s for (s, l) in labeled_sentences]\n",
    "labels = [int(l) for (s, l) in labeled_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeled_sentences\n",
    "# sentences\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 5271\n"
     ]
    }
   ],
   "source": [
    "# tokenize sentences: Convert words into numbers\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "vocab_size = len(tokenizer.word_counts)\n",
    "print(\"vocabulary size: {:d}\".format(vocab_size))\n",
    "\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v:k for (k, v) in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_lengths = np.array([len(s.split()) for s in sentences])\n",
    "# print([(p, np.percentile(seq_lengths, p)) for p \n",
    "#     in [75, 80, 90, 95, 99, 100]])\n",
    "# [(75, 16.0), (80, 18.0), (90, 22.0), (95, 26.0), (99, 36.0), (100, 71.0)]\n",
    "max_seqlen = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 04:42:46.451885: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# create dataset\n",
    "# Each sequence has as its lenght the number of words.\n",
    "sentences_as_ints = tokenizer.texts_to_sequences(sentences)\n",
    "# All sequences has lenght max_seqlen.\n",
    "sentences_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sentences_as_ints, maxlen=max_seqlen)\n",
    "labels_as_ints = np.array(labels)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (sentences_as_ints, labels_as_ints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SkipDataset shapes: ((64,), ()), types: (tf.int32, tf.int64)>\n",
      "<BatchDataset shapes: ((None, 64), (None,)), types: (tf.int32, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "# split into train and test\n",
    "dataset = dataset.shuffle(10000)\n",
    "test_size = len(sentences) // 3\n",
    "val_size = (len(sentences) - test_size) // 10\n",
    "test_dataset = dataset.take(test_size)\n",
    "val_dataset = dataset.skip(test_size).take(val_size)\n",
    "train_dataset = dataset.skip(test_size + val_size)\n",
    "print(train_dataset)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "print(train_dataset)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sentiment_analysis_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  337408    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  multiple                 66048     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  8256      \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 411,777\n",
      "Trainable params: 411,777\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "# vocab_size + 1 to account for PAD character\n",
    "model = SentimentAnalysisModel(vocab_size+1, max_seqlen)\n",
    "model.build(input_shape=(batch_size, max_seqlen))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 4s 57ms/step - loss: 0.6898 - accuracy: 0.5656 - val_loss: 0.6735 - val_accuracy: 0.6450\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 1s 36ms/step - loss: 0.6002 - accuracy: 0.7317 - val_loss: 0.4143 - val_accuracy: 0.8700\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 1s 36ms/step - loss: 0.3562 - accuracy: 0.8667 - val_loss: 0.2514 - val_accuracy: 0.9050\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 1s 36ms/step - loss: 0.2097 - accuracy: 0.9267 - val_loss: 0.1495 - val_accuracy: 0.9350\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 1s 37ms/step - loss: 0.1431 - accuracy: 0.9578 - val_loss: 0.0735 - val_accuracy: 0.9750\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 1s 36ms/step - loss: 0.1028 - accuracy: 0.9683 - val_loss: 0.0718 - val_accuracy: 0.9800\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 1s 37ms/step - loss: 0.0706 - accuracy: 0.9783 - val_loss: 0.0494 - val_accuracy: 0.9850\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 1s 36ms/step - loss: 0.0631 - accuracy: 0.9817 - val_loss: 0.0753 - val_accuracy: 0.9750\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 1s 37ms/step - loss: 0.0730 - accuracy: 0.9789 - val_loss: 0.0298 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 1s 36ms/step - loss: 0.0503 - accuracy: 0.9878 - val_loss: 0.0301 - val_accuracy: 0.9900\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "best_model_file = os.path.join(data_dir, \"best_model.h5\")\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(best_model_file,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True)\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\n",
    "num_epochs = 10\n",
    "print(train_dataset)\n",
    "history = model.fit(train_dataset, epochs=num_epochs, \n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[checkpoint, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 12ms/step - loss: 0.0470 - accuracy: 0.9850\n",
      "test loss: 0.047, test accuracy: 0.985\n"
     ]
    }
   ],
   "source": [
    "# evaluate with test set\n",
    "best_model = SentimentAnalysisModel(vocab_size+1, max_seqlen)\n",
    "best_model.build(input_shape=(batch_size, max_seqlen))\n",
    "best_model.load_weights(best_model_file)\n",
    "best_model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "test_loss, test_acc = best_model.evaluate(test_dataset)\n",
    "print(\"test loss: {:.3f}, test accuracy: {:.3f}\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0\tthis is a chilly unremarkable movie about an author living working in a chilly abstruse culture\n",
      "0\t0\tthe worst piece of crap ever along with the verizon customer service\n",
      "1\t1\tit's a gloriously fun fast paced and fairly accurate portrayal of the night of a raver\n",
      "0\t0\tbad characters bad story and bad acting\n",
      "1\t1\ti enjoyed reading this book to my children when they were little\n",
      "1\t1\ti own 2 of these cases and would order another\n",
      "1\t1\twe would recommend these to others\n",
      "1\t1\tthe selection on the menu was great and so were the prices\n",
      "1\t1\tgets the job done\n",
      "1\t1\ttom wilkinson broke my heart at the end and everyone else's judging by the amount of fumbling for hankies and hands going up to faces among males and females alike\n",
      "1\t1\tmark my words this is one of those cult films like evil dead 2 or phantasm that people will still be discovering and falling in love with 20 30 40 years down the line\n",
      "1\t1\tthe film's dialogue is natural real to life\n",
      "1\t1\thigh quality chicken on the chicken caesar salad\n",
      "1\t1\tworks great\n",
      "0\t0\tdon't waste your on this one\n",
      "1\t1\tyou get extra minutes so that you can carry out the call and not get cut off\n",
      "1\t1\tthis film highlights the fundamental flaws of the legal process that it's not about discovering guilt or innocence but rather is about who presents better in court\n",
      "1\t1\tbut it wasn't until i watched this film that i realised how great he actually was\n",
      "0\t0\twhat a mistake\n",
      "0\t0\ti can't tell you how disappointed i was\n",
      "1\t1\tvirgin wireless rocks and so does this cheap little phone\n",
      "0\t0\techo problem very unsatisfactory\n",
      "0\t0\tthis phone tries very hard to do everything but fails at it's very ability to be a phone\n",
      "0\t0\tif you plan to use this in a car forget about it\n",
      "1\t1\tgreat value\n",
      "1\t1\tyou cant go wrong with any of the food here\n",
      "0\t0\tif i take a picture the battery drops a bar and starts beeping letting me know its dieing\n",
      "1\t1\tthink of the film being like a dream\n",
      "1\t1\teverything about it is fine and reasonable for the price i e\n",
      "1\t1\tthe handsfree part works fine but then the car tries to download the address book and the treo reboots overall i still rate this device high\n",
      "1\t1\thow awesome is that\n",
      "0\t0\tdon't do it\n",
      "0\t0\thowever the ear pads come off easily and after only one week i lost one\n",
      "1\t1\teclectic selection\n",
      "1\t1\thowever paul schrader has indeed made a film about mishima that is both superb complex\n",
      "1\t1\tcall me a nut but i think this is one of the best movies ever\n",
      "1\t1\talso its slim enough to fit into my alarm clock docking station without removing the case\n",
      "1\t1\ti use this product in a motor control center where there is a lot of high voltage humming from the equipment and it works great\n",
      "0\t0\tabsolutel junk\n",
      "1\t1\tkrussel always good\n",
      "0\t0\ti'm still trying to get over how bad it was\n",
      "1\t1\tintegrated seamlessly with the motorola razr phone\n",
      "0\t0\tobviously they have a terrible customer service so you get what you pay for\n",
      "1\t1\tit plays louder than any other speaker of this size the price is so low that most would think the quality is lacking however it's not\n",
      "0\t0\tthe case is a flimsy piece of plastic and has no front or side protection whatsoever\n",
      "0\t0\ti wouldn't return\n",
      "0\t0\tthe death row scenes were entirely unmoving\n",
      "1\t1\trazr battery good buy\n",
      "1\t1\teach track commands sentiment actually contributing to the scenes and characters\n",
      "0\t0\tfirst off the reception sucks i have never had more than 2 bars ever\n",
      "0\t0\ti'm not sure what he was trying to do with this film\n",
      "0\t0\tthere is no real plot\n",
      "0\t0\ti've had better bagels from the grocery store\n",
      "0\t0\ti am far from a sushi connoisseur but i can definitely tell the difference between good food and bad food and this was certainly bad food\n",
      "0\t1\twon't go back\n",
      "1\t1\tthis is the number one best th game in the series\n",
      "0\t0\tthere was hardly any meat\n",
      "1\t1\ti can say that the desserts were yummy\n",
      "1\t0\tthe soundtrack wasn't terrible either\n",
      "0\t0\tyou also cannot take pictures with it in the case because the lense is covered\n",
      "1\t1\tbattery life is real good\n",
      "1\t1\twould come back again if i had a sushi craving while in vegas\n",
      "0\t0\tthere was nothing believable about it at all\n",
      "0\t0\tthe fish is badly made and some of its underwater shots are repeated a thousand times in the film\n"
     ]
    }
   ],
   "source": [
    "# predict on batches\n",
    "labels, predictions = [], []\n",
    "idx2word[0] = \"PAD\"\n",
    "is_first_batch = True\n",
    "for test_batch in test_dataset:\n",
    "    inputs_b, labels_b = test_batch\n",
    "    pred_batch = best_model.predict(inputs_b)\n",
    "    predictions.extend([(1 if p > 0.5 else 0) for p in pred_batch])\n",
    "    labels.extend([l for l in labels_b])\n",
    "    if is_first_batch:\n",
    "        for rid in range(inputs_b.shape[0]):\n",
    "            words = [idx2word[idx] for idx in inputs_b[rid].numpy()]\n",
    "            words = [w for w in words if w != \"PAD\"]\n",
    "            sentence = \" \".join(words)\n",
    "            print(\"{:d}\\t{:d}\\t{:s}\".format(labels[rid], predictions[rid], sentence))\n",
    "        is_first_batch = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.980\n",
      "confusion matrix\n",
      "[[482  14]\n",
      " [  6 498]]\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy score: {:.3f}\".format(accuracy_score(labels, predictions)))\n",
    "print(\"confusion matrix\")\n",
    "print(confusion_matrix(labels, predictions))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_p39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5927ddc4cb01a6ba059eb42a698e965db03a50bdd8d02bc22bb15431b0c99791"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
